>  ### 损失函数选择

PyTorch中提供了许多常用的损失函数，用于评估模型预测与真实值之间的差距。这些函数可以分为以下几类：

**1. 回归任务损失函数(适于预测连续值)**

- **L1 Loss（平均绝对误差，MAE）**：计算预测值与真实值之间的绝对值之和的平均值。公式为：

```
L1Loss = 1/n * Σ|y_true - y_pred|
```

其中，n为样本数，y_true为真实值，y_pred为预测值。

- **MSE（均方误差）**：计算预测值与真实值之间的平方差的平均值。公式为：

```
MSE = 1/n * Σ(y_true - y_pred)^2
```

- **SmoothL1 Loss**：是L1 Loss和MSE的平滑组合，在[-1, 1]区间内使用MSE，其余区间使用L1 Loss。公式为：

```
SmoothL1Loss = 0.5 * x^2 * beta  if |x| < 1 else |x| - 0.5 + beta/2
```

其中，x为预测值与真实值之差，beta为控制平滑程度的参数。

**2. 分类任务损失函数(适于预测离散值)**

- **交叉熵损失**：用于多分类任务，计算模型预测的概率分布与真实分布之间的交叉熵。公式为：

```
CrossEntropyLoss = - Σp(c_true) * log(p(c_pred))
```

其中，c_true为真实类别，c_pred为预测类别，p为概率分布。

- **BCELoss（二分类交叉熵损失）**：用于二分类任务，计算模型预测的概率与真实标签（0/1）之间的交叉熵。公式为：

```
BCELoss = - Σy_true * log(y_pred) + (1 - y_true) * log(1 - y_pred)
```

- **NLLLoss（负对数似然损失）**：用于多分类任务，计算模型预测的对数似然之和的负值。公式为：

```
NLLLoss = - Σlog(p(c_true))
```

其中，p(c_true)为真实类别的预测概率。

**3. 其他损失函数**

- **Hinge Loss**：用于最大间距分类，旨在将不同类别的样本拉开最大距离。
- **KLDivLoss（Kullback-Leibler 散度）**：用于衡量两个概率分布之间的差异。
- **MSELoss2d**：用于计算二维张量之间的MSE。

**选择合适的损失函数**

选择合适的损失函数取决于任务类型和数据集的特性。一般来说，对于回归任务，MSE或SmoothL1 Loss通常是较好的选择；对于多分类任务，交叉熵损失是常用的选择；对于二分类任务，BCELoss或NLLLoss可以根据需要选择。

> ### 优化器选择

**常用的 PyTorch 优化器：**

- **SGD（随机梯度下降）**：是最简单、最常用的优化器之一。它通过每次更新模型参数的方向来减小损失函数值。SGD 适用于各种任务，并且通常是超参数调整的良好起点。
- **Adam（自适应矩估计）**：是一种自适应学习率优化器，它能够自动调整每个参数的学习率，从而提高训练效率。Adam 适用于具有大量参数的模型，并且通常可以比 SGD 更快地收敛。
- **RMSprop（根均方误差传播）**：另一种自适应学习率优化器，它通过跟踪每个参数的梯度平方来调整学习率。RMSprop 适用于处理稀疏数据或具有梯度幅度变化较大的任务。
- **Adagrad（自适应梯度）**：类似于 RMSprop，但它使用历史梯度平方和的累积值来调整学习率。Adagrad 适用于处理稀疏数据或具有梯度幅度变化较大的任务。

**如何选择合适的优化器？**

- **任务类型**：不同的任务可能需要不同的优化器。例如，对于回归任务，SGD 或 Adam 可能是不错的选择；对于分类任务，Adam 或 RMSprop 可能是更好的选择。
- **模型大小**：对于大型模型，Adam 通常比 SGD 更有效。
- **数据集**：对于稀疏数据，Adagrad 或 RMSprop 可能是更好的选择。
- **计算资源**：某些优化器（例如 Adam）比其他优化器（例如 SGD）需要更多的计算资源。